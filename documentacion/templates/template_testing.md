# üß™ TEMPLATE DE TESTING

**Archivo:** `template_testing.md`  
**Prop√≥sito:** Template estandarizado para testing de componentes del sistema trading

---

## üìã **ESTRUCTURA DE TESTING**

### **üéØ Tipos de Tests Requeridos**
```
Tests por Componente:
‚îú‚îÄ‚îÄ üîß Unit Tests - Funciones individuales
‚îú‚îÄ‚îÄ üîó Integration Tests - Integraci√≥n con otros componentes
‚îú‚îÄ‚îÄ ‚ö° Performance Tests - Tiempo y memoria
‚îú‚îÄ‚îÄ üìä Data Tests - Validaci√≥n con datos reales MT5
‚îî‚îÄ‚îÄ üéØ Regression Tests - No romper funcionalidad existente
```

---

## üß™ **TEMPLATE TEST UNITARIO**

### **üìù Archivo: `test_[componente].py`**
```python
"""
Tests unitarios para [NombreComponente]
Sistema Trading Grid
Fecha: [DD/MM/AAAA]
"""

import pytest
import pandas as pd
import numpy as np
import time
from unittest.mock import Mock, patch
from datetime import datetime, timedelta

# Import del componente a testear
from [archivo_componente] import [NombreComponente]

class Test[NombreComponente]:
    """Suite de tests para [NombreComponente]"""
    
    @pytest.fixture
    def componente(self):
        """Fixture del componente con configuraci√≥n de test"""
        config_test = {
            'parametro1': 'test_value',
            'parametro2': 50,
            'test_mode': True
        }
        return [NombreComponente](config_test)
    
    @pytest.fixture
    def datos_validos(self):
        """Fixture con datos v√°lidos para testing"""
        return pd.DataFrame({
            'timestamp': pd.date_range('2025-01-01', periods=100, freq='1min'),
            'open': np.random.uniform(1.1000, 1.1100, 100),
            'high': np.random.uniform(1.1000, 1.1100, 100),
            'low': np.random.uniform(1.1000, 1.1100, 100),
            'close': np.random.uniform(1.1000, 1.1100, 100),
            'volume': np.random.randint(100, 1000, 100)
        })
    
    @pytest.fixture  
    def datos_invalidos(self):
        """Fixture con datos inv√°lidos para testing"""
        return pd.DataFrame({
            'columna_incorrecta': [1, 2, 3]
        })

    # ===== TESTS B√ÅSICOS =====
    
    def test_inicializacion(self, componente):
        """Test que el componente se inicializa correctamente"""
        assert componente is not None
        assert hasattr(componente, 'config')
        assert hasattr(componente, 'logger')
        assert componente.config['test_mode'] is True
    
    def test_configuracion_default(self):
        """Test configuraci√≥n por defecto"""
        componente = [NombreComponente]()
        config = componente._get_default_config()
        
        assert isinstance(config, dict)
        assert 'parametro1' in config
        assert 'parametro2' in config
    
    # ===== TESTS FUNCIONALIDAD =====
    
    def test_funcion_principal_datos_validos(self, componente, datos_validos):
        """Test funci√≥n principal con datos v√°lidos"""
        resultado = componente.funcion_principal(datos_validos)
        
        assert isinstance(resultado, dict)
        assert 'status' in resultado
        assert resultado['status'] == 'success'
        assert 'timestamp' in resultado
        assert 'data_processed' in resultado
        assert resultado['data_processed'] == len(datos_validos)
    
    def test_funcion_principal_datos_vacios(self, componente):
        """Test funci√≥n principal con DataFrame vac√≠o"""
        datos_vacios = pd.DataFrame()
        
        with pytest.raises(ValueError, match="Los datos no pueden estar vac√≠os"):
            componente.funcion_principal(datos_vacios)
    
    def test_validacion_datos_invalidos(self, componente, datos_invalidos):
        """Test validaci√≥n con datos inv√°lidos"""
        with pytest.raises(ValueError, match="Columna requerida"):
            componente.funcion_principal(datos_invalidos)
    
    # ===== TESTS PERFORMANCE =====
    
    def test_performance_tiempo_ejecucion(self, componente, datos_validos):
        """Test que el tiempo de ejecuci√≥n sea < 5 segundos"""
        start_time = time.time()
        componente.funcion_principal(datos_validos)
        execution_time = time.time() - start_time
        
        assert execution_time < 5.0, f"Ejecuci√≥n tom√≥ {execution_time:.2f}s (>5s)"
    
    @pytest.mark.performance  
    def test_performance_memoria(self, componente, datos_validos):
        """Test uso de memoria durante ejecuci√≥n"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memoria_inicial = process.memory_info().rss / 1024 / 1024  # MB
        
        componente.funcion_principal(datos_validos)
        
        memoria_final = process.memory_info().rss / 1024 / 1024  # MB
        incremento_memoria = memoria_final - memoria_inicial
        
        assert incremento_memoria < 100, f"Incremento memoria: {incremento_memoria:.2f}MB"
    
    def test_performance_multiples_ejecuciones(self, componente, datos_validos):
        """Test performance con m√∫ltiples ejecuciones"""
        tiempos = []
        
        for _ in range(10):
            start = time.time()
            componente.funcion_principal(datos_validos)
            tiempos.append(time.time() - start)
        
        tiempo_promedio = np.mean(tiempos)
        tiempo_maximo = np.max(tiempos)
        
        assert tiempo_promedio < 3.0, f"Tiempo promedio: {tiempo_promedio:.2f}s"
        assert tiempo_maximo < 5.0, f"Tiempo m√°ximo: {tiempo_maximo:.2f}s"
    
    # ===== TESTS EDGE CASES =====
    
    def test_datos_extremos_pequenos(self, componente):
        """Test con dataset muy peque√±o"""
        datos_pequenos = pd.DataFrame({
            'timestamp': [datetime.now()],
            'close': [1.1000]
        })
        
        # Dependiendo del componente, esto puede ser v√°lido o inv√°lido
        # Ajustar seg√∫n l√≥gica espec√≠fica
        try:
            resultado = componente.funcion_principal(datos_pequenos)
            assert isinstance(resultado, dict)
        except ValueError as e:
            # Si requiere m√≠nimo de datos, verificar mensaje apropiado
            assert "m√≠nimo" in str(e).lower()
    
    def test_datos_con_nulos(self, componente):
        """Test con datos que contienen NaN"""
        datos_con_nulos = pd.DataFrame({
            'timestamp': pd.date_range('2025-01-01', periods=10),
            'close': [1.1000, np.nan, 1.1020, np.nan, 1.1040] * 2
        })
        
        # El componente debe manejar NaN apropiadamente
        resultado = componente.funcion_principal(datos_con_nulos)
        assert isinstance(resultado, dict)
    
    # ===== TESTS CONFIGURACI√ìN =====
    
    def test_configuracion_personalizada(self):
        """Test con configuraci√≥n personalizada"""
        config_custom = {
            'parametro1': 'valor_personalizado',
            'parametro2': 999
        }
        
        componente = [NombreComponente](config_custom)
        assert componente.config['parametro1'] == 'valor_personalizado'
        assert componente.config['parametro2'] == 999
    
    # ===== TESTS INTEGRACI√ìN =====
    
    @patch('data_logger.DataLogger')
    def test_integracion_data_logger(self, mock_logger, componente, datos_validos):
        """Test integraci√≥n con DataLogger"""
        componente.funcion_principal(datos_validos)
        
        # Verificar que se llam√≥ al logger
        assert mock_logger.called
        # Verificar m√©todos espec√≠ficos seg√∫n implementaci√≥n
    
    # ===== TESTS STATUS Y MONITORING =====
    
    def test_get_status(self, componente):
        """Test funci√≥n get_status"""
        status = componente.get_status()
        
        assert isinstance(status, dict)
        assert 'component' in status
        assert 'status' in status
        assert status['component'] == componente.__class__.__name__
        assert status['status'] == 'active'


# ===== TESTS DE INTEGRACI√ìN =====

class TestIntegracion[NombreComponente]:
    """Tests de integraci√≥n del componente con el sistema"""
    
    def test_integracion_con_main(self):
        """Test integraci√≥n con controlador principal"""
        # Simular integraci√≥n con main.py
        pass
    
    def test_integracion_config_global(self):
        """Test integraci√≥n con config.py global"""
        from config import *
        componente = [NombreComponente]()
        
        # Verificar que usa configuraci√≥n global apropiadamente
        assert componente.config is not None


# ===== TESTS DE DATOS REALES =====

@pytest.mark.integration
class TestDatosReales[NombreComponente]:
    """Tests con datos reales de MT5"""
    
    @pytest.fixture
    def datos_mt5_reales(self):
        """Fixture con datos reales de MT5 (si est√°n disponibles)"""
        # Cargar datos desde archivo CSV o MT5
        try:
            datos = pd.read_csv('data/2025-08-08/velas_EURUSD_M15_6semanas.csv')
            return datos
        except FileNotFoundError:
            pytest.skip("Datos reales no disponibles")
    
    def test_con_datos_reales_mt5(self, componente, datos_mt5_reales):
        """Test con datos reales de MT5"""
        if datos_mt5_reales is not None:
            resultado = componente.funcion_principal(datos_mt5_reales)
            
            assert resultado['status'] == 'success'
            assert resultado['data_processed'] > 0
    
    def test_precision_con_datos_historicos(self, componente, datos_mt5_reales):
        """Test de precisi√≥n con datos hist√≥ricos"""
        if datos_mt5_reales is not None:
            resultado = componente.funcion_principal(datos_mt5_reales)
            
            # Verificar m√©tricas de precisi√≥n espec√≠ficas del componente
            # Esto depende del tipo de componente (indicador, se√±ales, etc.)
            assert 'precision' in resultado or 'accuracy' in resultado


# ===== CONFIGURACI√ìN PYTEST =====

# Marcadores personalizados para organizar tests
pytestmark = [
    pytest.mark.unit,  # Tests unitarios b√°sicos
]

# Configuraci√≥n de performance tests
def pytest_configure(config):
    """Configuraci√≥n de pytest"""
    config.addinivalue_line(
        "markers", "performance: marca tests de performance"
    )
    config.addinivalue_line(
        "markers", "integration: marca tests de integraci√≥n"
    )

# Funciones de utilidad para tests
def crear_datos_ohlc(n_rows=100, symbol="EURUSD"):
    """Utilidad para crear datos OHLC de prueba"""
    base_price = 1.1000
    dates = pd.date_range('2025-01-01', periods=n_rows, freq='1min')
    
    # Generar precios realistas
    returns = np.random.normal(0, 0.0001, n_rows)
    prices = base_price * (1 + returns).cumprod()
    
    return pd.DataFrame({
        'timestamp': dates,
        'symbol': symbol,
        'open': prices,
        'high': prices * (1 + np.abs(np.random.normal(0, 0.0005, n_rows))),
        'low': prices * (1 - np.abs(np.random.normal(0, 0.0005, n_rows))),
        'close': prices,
        'volume': np.random.randint(100, 1000, n_rows)
    })

if __name__ == "__main__":
    # Ejecutar tests espec√≠ficos
    pytest.main([__file__, "-v", "--tb=short"])
```

---

## ‚ö° **CHECKLIST DE TESTING**

### **‚úÖ Tests Obligatorios**
- [ ] **Test inicializaci√≥n** - Constructor y configuraci√≥n
- [ ] **Test funci√≥n principal** - Casos v√°lidos e inv√°lidos  
- [ ] **Test validaci√≥n datos** - Entrada correcta/incorrecta
- [ ] **Test performance** - < 5 segundos y memoria
- [ ] **Test edge cases** - Casos l√≠mite y extremos
- [ ] **Test configuraci√≥n** - Default y personalizada
- [ ] **Test integraci√≥n** - Con otros componentes

### **üìä Coverage Objetivo**
- **L√≠neas de c√≥digo:** > 80% coverage
- **Funciones:** 100% funciones testeadas
- **Branches:** > 70% branch coverage
- **Edge cases:** Casos principales cubiertos

### **üéØ M√©tricas de Calidad**
- **Tests passing:** 100% tests deben pasar
- **Tiempo total:** < 30 segundos suite completa
- **Memoria m√°xima:** < 200 MB durante tests
- **Flaky tests:** 0 tests intermitentes

---

## üîß **COMANDOS DE TESTING**

### **üß™ Ejecutar Tests**
```bash
# Tests espec√≠ficos del componente
pytest test_componente.py -v

# Tests con coverage
pytest test_componente.py --cov=componente --cov-report=html

# Solo tests r√°pidos (sin integration)
pytest test_componente.py -m "not integration" 

# Solo performance tests
pytest test_componente.py -m performance

# Tests en paralelo
pytest test_componente.py -n auto
```

### **üìä An√°lisis de Resultados**
```bash
# Reporte detallado
pytest test_componente.py --tb=long --capture=no

# Benchmark de performance  
pytest test_componente.py --benchmark-only

# Profiling de memoria
pytest test_componente.py --memray
```

---

## üìù **DOCUMENTACI√ìN POST-TESTING**

### **‚úÖ Actualizar Despu√©s de Testing**
1. **Bit√°cora:** Registrar resultados de tests
2. **M√©tricas:** Documentar coverage y performance
3. **Problemas:** Registrar bugs encontrados
4. **Mejoras:** Identificar optimizaciones

### **üìä Template Reporte Testing**
```markdown
## üß™ REPORTE TESTING - [ComponenteNombre]

### Resultados:
- Tests ejecutados: [X]/[Total]
- Tests pasando: [X]/[Total] ([X]%)
- Coverage: [X]%
- Tiempo total: [X]s

### Performance:
- Tiempo promedio: [X]s
- Memoria m√°xima: [X]MB
- Throughput: [X] ops/s

### Issues encontrados:
- [ ] Issue 1: [Descripci√≥n]
- [ ] Issue 2: [Descripci√≥n]

### Pr√≥ximos pasos:
- [ ] Optimizaci√≥n 1
- [ ] Fix bug 1
```

---

*Template Testing v1.0 - √öltima actualizaci√≥n: Agosto 10, 2025*
